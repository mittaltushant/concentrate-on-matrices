% Autogenerated translation of talk1.md by Texpad
% To stop this file being overwritten during the typeset process, please move or remove this header

\documentclass[12pt]{book}
\usepackage{graphicx}
\usepackage{fontspec}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,left=.5in,right=.5in,top=.3in,bottom=0.3in]{geometry}
\setlength\parindent{0pt}
\setlength{\parskip}{\baselineskip}
\setmainfont{Helvetica Neue}
\usepackage{hyperref}
\pagestyle{plain}
\begin{document}

\hrule
title: Matrix Chernoff Bounds for sums of independent random variables 
date: 2023-01-26
category: notes
speaker: Tushant Mittal

\section*{abstract: "We do a matrix-version of our dear Chernoff bound."}

\section*{Summary}

In this talk, we proved a matrix-version of a simple Chernoff bound. The scalar version which we will generalize is the following. 

\subsection*{Scalar Chernoff}

Let \$X = sum\emph{i eps}ia\emph{i \$ where \$a}i in R\$ are fixed, and, \$eps\emph{i\$ are \$pm 1\$ Rademacher random variables. Then,
\textbackslash{}[ prob\{sum}i eps\emph{ia}i $>$ t\} \textasciitilde{}leq\textasciitilde{} e\textasciicircum{}\{frac\{-t\textasciicircum{}2\}\{2sigma\textasciicircum{}2\}\},\textbackslash{};\textbackslash{}; sigma\textasciicircum{}2 = ex\{X\textasciicircum{}2\} =\textbackslash{}; sum\emph{i a}i\textasciicircum{}2
\textbackslash{}]
Let's look at a standard proof of this to see where it breaks down. For a single Rademacher variable, one can show that \$ex\{e\textasciicircum{}\{theta eps a\}\} = cosh(theta a) leq e\textasciicircum{}\{frac\{theta\textasciicircum{}2 a\textasciicircum{}2\}\{2\}\}\$. The goal is to break the sum on \$n\$ such variables as a product of these. Fix \$theta $>$ 0in R\$.
\textbackslash{}[
begin\{align\}
prob\{X $>$ t\} \textasciitilde{}\&=\textasciitilde{} prob\{e\textasciicircum{}\{theta X\} $>$ e\textasciicircum{}\{theta t\}\}\textbackslash{};\textbackslash{};small\{text\{ (Monotonicity of exponential)\} \}\textbackslash{}
 \&leq e\textasciicircum{}\{-theta t\}\textbackslash{},ex\{ e\textasciicircum{}\{theta X\}\} \textbackslash{};\textbackslash{};small\{text\{ (Markov)\} \}\textbackslash{}
 \textasciitilde{}\&=\textasciitilde{} e\textasciicircum{}\{-theta t\}\textbackslash{},ex\{ prod\emph{i e\textasciicircum{}\{theta eps}i a\emph{i\}\} \textbackslash{};\textbackslash{}; small\{(e\textasciicircum{}\{a+b\} = e\textasciicircum{}acdot e\textasciicircum{}b)\} \textbackslash{}
 \textasciitilde{}\&=\textasciitilde{} e\textasciicircum{}\{-theta t\}\textbackslash{},prod}iex\{ e\textasciicircum{}\{theta eps\emph{i a}i\}\} \textbackslash{};\textbackslash{}; small\{text\{ (Independence)\} \} \textbackslash{}
 \textasciitilde{}\&leq\textasciitilde{} e\textasciicircum{}\{-theta t\}\textbackslash{},prod\emph{i e\textasciicircum{}\{frac\{theta\textasciicircum{}2 a}i\textasciicircum{}2\}\{2\}\}  \textbackslash{};\textbackslash{}; small\{text\{ (Plugging in the 1-variable bound )\} \}\textbackslash{}
 \textasciitilde{}\&leq\textasciitilde{} e\textasciicircum{}\{frac\{-t\textasciicircum{}2\}\{2(sum\emph{i a}i\textasciicircum{}2)\}\}  \textbackslash{};\textbackslash{}; small\{text\{ (Optimizing for \}theta) \}
end\{align\}
\textbackslash{}]

\subsection*{Matrix Chernoff}

We will generalize the above setup as follows.
Let \$X = sum\emph{i eps}iA\emph{i \$ where \$A}i\$ are fixed Hermitian matrices, and, \$eps\emph{i\$ are \$pm 1\$ Rademacher random variables. Then,
\textbackslash{}[ prob\{lambda}\{max\}(X) $>$ t\} \textasciitilde{}leq\textasciitilde{} e\textasciicircum{}\{frac\{-t\textasciicircum{}2\}\{2sigma\textasciicircum{}2\}\} 
\textbackslash{}]

We will give two proofs each of which give a slightly different \$sigma\$. The first question is to make sense of exponentials of matrix valued random variables. This, fortunately, is easy. 

\subsection*{Lifting functions to matrices}

Let \$f: I to R\$ be a function defined on an interval \$I\$. Let \$A = QLambda Q\textasciicircum{}* \$ be an Hermitian matrix such that all eigenvalues of \$A\$ lies in \$I\$, i.e., \$mathrm\{Spec\}(A)subseteq I\$. Then, we can define \$f(A) := Qf(Lambda) Q\textasciicircum{}*\$, where \$f(Lambda)\$ is obtained by applying \$f\$ entry-wise to the diagonal matrix \$Lambda\$, i.e., \$f(Lambda) = mathrm\{diag\}(f(lambda\emph{1), cdots, f(lambda}d))\$.

\subsection*{Mimicking the scalar proof}

 One can repeat the scalar argument for the first two steps but it is not clear how to handle the term, \$ex\{lambda\emph{\{max\}(e\textasciicircum{}\{theta X\})\}\$. Ideally, we would like to have \$prod}i ex\{lambda\emph{max (e\textasciicircum{}\{theta eps}i A\_i\})\}\$. The key difficulty is that \$e\textasciicircum{}\{A+B\}neq e\textasciicircum{}Ae\textasciicircum{}B\$ for matrices. This can be resolved (by at least) two approaches. 

\textbackslash{}[
begin\{align\}
small\{[text\{Naive Hope\}]\}\textbackslash{};\textbackslash{};\textbackslash{}; \&ex\{lambda\emph{\{max\}(e\textasciicircum{}\{theta X\})\} \textasciitilde{}=\textasciitilde{} ex\{lambda}\{max\}left(prod\emph{i e\textasciicircum{}\{theta eps}i A\emph{i\}right)\} \textasciitilde{}leq\textasciitilde{} prod}i ex\{lambda\emph{max (e\textasciicircum{}\{theta eps}i A\emph{i\})\}  \textbackslash{}
small\{[text\{Ahlswede-Winter\}]\}\textbackslash{};\textbackslash{};\textbackslash{}; \&ex\{lambda}\{max\}(e\textasciicircum{}\{theta X\})\} \textasciitilde{}leq\textasciitilde{} ex\{tr(e\textasciicircum{}\{theta X\})\} \textasciitilde{}leq\textasciitilde{}  dprod\emph{i ex\{lambda}max (e\textasciicircum{}\{theta eps\emph{i A}i\})\}\textbackslash{}
small\{[text\{Tropp\}]\}\textbackslash{};\textbackslash{};\textbackslash{}; \&ex\{lambda\emph{\{max\}(e\textasciicircum{}\{theta X\})\} \textasciitilde{}leq\textasciitilde{} ex\{tr(e\textasciicircum{}\{theta X\})\} \textasciitilde{}leq\textasciitilde{}   tr\textbackslash{}, mathrm\{exp\}left( sum}i log ex\{(e\textasciicircum{}\{theta eps\emph{i A}i\})\}right) \textbackslash{}
end\{align\}
\textbackslash{}]

\subsection*{Using the trace inequalities}

\begin{itemize}
\item [AW] By definition of the matrix exponential, \$lambda\emph{max(e\textasciicircum{}A) = e\textasciicircum{}\{lambda}max(A)\}\$. Thus, treating \$lambda\emph{max(A)\$ as a scalar random variable, we can plug it into the scalar inequality we used earlier, \$ex\{e\textasciicircum{}\{theta eps a\}\} leq e\textasciicircum{}\{frac\{theta\textasciicircum{}2 a\textasciicircum{}2\}\{2\}\}\$. Thus, we get, \$ex\{lambda}max (e\textasciicircum{}\{theta eps\emph{i A}i\})\} leq e\textasciicircum{}\{frac\{theta\textasciicircum{}2 lambda\emph{max(A}i)\textasciicircum{}2\}\{2\}\}\$. It is now exactly like the scalar Chernoff and we get a variance term of \$sum\emph{i lambda}max(A\_i)\textasciicircum{}2\$.
\item [Tropp] We need two more facts. 

\begin{itemize}
\item Firstly, a matrix version of the 1-variable inequality. This is given by \$log ex\{e\textasciicircum{}theta eps\emph{i A}i\} preceq frac\{theta\textasciicircum{}2 A\_i\textasciicircum{}2\}\{2\}\$. Here the order being used is the Loewner order (\$Apreceq B\$ if \$B-A\$ is PSD). The proof is analogous to the scalar proof and is given in Tropp's book [].
\item The fact that trace-exponential is monotone in the sense that if \$Apreceq B\$, \$tr\textbackslash{}, e\textasciicircum{}A leq tr\textbackslash{}, e\textasciicircum{}B\$. This is easy to establish by using the fact that \$Apreceq B\$ implies \$lambda\emph{i(A)leq lambda}i(B)\$ which itself follows by Courant-Fischer. 
\end{itemize}
\end{itemize}

Now, we are ready. 
\textbackslash{}[
begin\{align\}
log ex\{e\textasciicircum{}theta eps\emph{i A}i\} \&\textasciitilde{}preceq\textasciitilde{} frac\{theta\textasciicircum{}2 A\emph{i\textasciicircum{}2\}\{2\}\textbackslash{};\textbackslash{};\textbackslash{}; small\{text\{[Fact 1]\}\}\textbackslash{}
sum}i log ex\{e\textasciicircum{}theta eps\emph{i A}i\} \&\textasciitilde{}preceq\textasciitilde{} frac\{theta\textasciicircum{}2 sum\emph{i A}i\textasciicircum{}2\}\{2\}\textbackslash{}
tr \textbackslash{}; mathrm\{exp\}left(sum\emph{i log ex\{e\textasciicircum{}theta eps}i A\emph{i\}right) \&\textasciitilde{}leq\textasciitilde{} tr \textbackslash{}; mathrm\{exp\}left(frac\{theta\textasciicircum{}2 sum}i A\emph{i\textasciicircum{}2\}\{2\}right) \textbackslash{};\textbackslash{}; \textbackslash{}; small\{text\{[Fact 2]\}\}\textbackslash{}
\&\textasciitilde{}leq\textasciitilde{} d\textbackslash{}, lambda}max left( mathrm\{exp\}left( frac\{theta\textasciicircum{}2 sum\emph{i A}i\textasciicircum{}2\}\{2\}right)right)\textbackslash{}
\&\textasciitilde{}=\textasciitilde{} d\textbackslash{}, mathrm\{exp\}left( frac\{theta\textasciicircum{}2 lambda\emph{maxleft(sum}i A\emph{i\textasciicircum{}2right)\}\{2\}right).
end\{align\}
\textbackslash{}]
This, gives us a variance term of \$lambda}maxleft(sum\emph{i A}i\textasciicircum{}2right) \$ which is better than the earlier one of \$sum\emph{i lambda}maxleft(A\_iright)\textasciicircum{}2\$ by up to a factor of \$d\$. This matters as we have the factor of \$d\$ in the exponent.

\subsection*{Deriving the trace inequalities}

\begin{itemize}
\item [Golden-Thompson] Lie-Trotter formula says that \$e\textasciicircum{}\{A+B\} = lim\emph{\{nto infty\} \textbackslash{}left(e\textasciicircum{}\{A/n\}e\textasciicircum{}\{B/n\} \textbackslash{}right)\textasciicircum{}\{n\}\$. Such a formula can be used to derive the GT inequality, \$tr(e\textasciicircum{}\{A+B\})leq tr(e\textasciicircum{}Ae\textasciicircum{}B)\$. However, this inequality is false for three or more matrices. Ahlswede--Winter cleverly apply this in an iterative way by pairing this with the inequality 
\$tr(e\textasciicircum{}A e\textasciicircum{}B) leq lambda}max(e\textasciicircum{}A)tr(e\textasciicircum{}B)\$.
\item [Lieb's Concavity] Tropp's insight is that one must instead work with the cumulant generating function, \$log e\textasciicircum{}\{X\}\$. The advantage of this POV is that this approach generalizes to a much more general settings. Moreover, it gives tighter bounds. 
\end{itemize}

\section*{Resources}

\begin{itemize}
\item Tropp, Joel A. "An Introduction to Matrix Concentration Inequalities." \href{https://doi.org/10.48550/arXiv.1501.01571}{arXiv}. 
\item \href{https://www.math.uwaterloo.ca/~harvey/W11/Lecture11Notes.pdf}{Lecture Notes} on Ahlswede-Winter Inequality by Nicholas Harvey.
\item Garg, Ankit, Lee, Yin T., Song, Zhao, and Nikhil Srivastava. "A Matrix Expander Chernoff Bound." \href{https://doi.org/10.48550/arXiv.1704.03864}{arXiv}.
\item Talk by Joel Tropp at \href{https://www.youtube.com/watch?v=T9ViSznHeUE}{[Youtube Link]}
\end{itemize}

\end{document}
